# Schema Down-Conversion Algorithm â€” Formal Specification

**Session Type**: Problem Solving + New System
**Date**: 2026-02-07
**Council**: T (lead) + G | 4 rounds (C unavailable â€” upstream CLI issue)

## Problem Statement

We need a formal, deterministic algorithm that converts **any valid JSON Schema** (Draft 2020-12 or earlier) into the most faithful LLM-compatible projection possible. This algorithm becomes the core specification for an open-source project: a standalone schema-to-schema converter implemented as a Rust CLI with WASM/FFI bindings for TypeScript, Java, and Python.

**No existing tool does this.** All prior art (Instructor, Outlines, LangChain, Marvin, Guidance) operates at the code/wrapper level. This is a greenfield standalone schema transformer.

---

## The Algorithm: 8-Pass Compiler Pipeline

> [!IMPORTANT]
> JSON Schema is designed for **validation** (permissive â€” "is this valid?"). LLM structured output is designed for **generation** (restrictive â€” "what shape must I produce?"). The algorithm bridges this gap by making all implicit constraints explicit.

The algorithm targets **OpenAI Strict Mode** as the baseline compilation target (most constrained). Other providers (Gemini, Claude) are treated as supersets where specific passes can be relaxed or skipped.

### Pass 0: Schema Normalization

| Property        | Value                                                                                                                                                                     |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Detects**     | `$ref` pointers, draft version differences, `items` (array form), `definitions` keyword                                                                                   |
| **Transforms**  | Resolves all `$ref` to inline definitions; normalizes draft syntax; migrates `items` array â†’ `prefixItems` + `additionalItems` â†’ `items`; renames `definitions` â†’ `$defs` |
| **Lossy**       | No â€” purely structural                                                                                                                                                    |
| **Ordering**    | Must be **first** â€” all subsequent passes assume a fully resolved, self-contained schema                                                                                  |
| **Limitations** | Root-relative JSON Pointers only (`#/...`). `$id`/`$anchor` scoped resolution deferred.                                                                                   |

### Pass 1: Composition Compilation (`allOf` Merge)

| Property       | Value                                                                             |
| -------------- | --------------------------------------------------------------------------------- |
| **Detects**    | `allOf` nodes (schema inheritance/mixins)                                         |
| **Transforms** | Merges all `allOf` sub-schemas into a single flat object schema                   |
| **Lossy**      | Partially â€” complex constraint intersections approximated or dropped              |
| **Ordering**   | After Pass 0 (needs resolved refs); before Pass 2 (polymorphism needs clean objs) |

### Pass 2: Polymorphism Simplification (`oneOf`/`anyOf`)

| Property       | Value                                                       |
| -------------- | ----------------------------------------------------------- |
| **Detects**    | `oneOf` with/without `discriminator`; multi-variant `anyOf` |
| **Transforms** | Rewrites `oneOf` â†’ `anyOf` (OpenAI supports it natively)    |
| **Lossy**      | No â€” `anyOf` is semantically compatible for generation      |
| **Ordering**   | After Pass 1 (allOf merged); before Pass 3                  |

> [!IMPORTANT]
> **Round 4 Decision â€” `anyOf` over Flattening.**
> G proved that flattening causes discriminator hallucination (the "kafka listener" bug). OpenAI Strict supports `anyOf` natively. Using `anyOf` means the model **must** pick a variant branch â€” once it commits to `http`, kafka-only fields are physically excluded from its valid token set. This also eliminates token waste from nullable variant fields and flow phase confusion.

### Pass 3: Dictionary Transpilation (Map â†’ Array)

| Property       | Value                                                                             |
| -------------- | --------------------------------------------------------------------------------- |
| **Detects**    | `{type: object, additionalProperties: Schema}` (Map pattern); `patternProperties` |
| **Transforms** | `Map<K, V>` â†’ `Array<{key: K, value: V}>`. Nested maps â†’ multi-key arrays.        |
| **Lossy**      | Yes â€” structural shape changes. **Requires codec entry.**                         |
| **Ordering**   | After Pass 2; before Pass 4                                                       |

### Pass 4: Opaque Type Stringification

| Property       | Value                                                                                |
| -------------- | ------------------------------------------------------------------------------------ |
| **Detects**    | `{type: object}` with no `properties`; `{}` (any type)                               |
| **Transforms** | â†’ `{type: string, description: "JSON-encoded string..."}`. **Requires codec entry.** |
| **Ordering**   | After Pass 3 (maps handled); before Pass 6 (strict needs typed everything)           |

### Pass 5: Recursion Breaking

| Property       | Value                                                                       |
| -------------- | --------------------------------------------------------------------------- |
| **Detects**    | Recursive `$ref` cycles identified in Pass 0                                |
| **Transforms** | OpenAI: prune at depth limit â†’ opaque string. Gemini: skip (native support) |
| **Lossy**      | Yes â€” structure truncated at depth boundary                                 |

### Pass 6: Strict Mode Enforcement

| Property       | Value                                                                               |
| -------------- | ----------------------------------------------------------------------------------- | ----- |
| **Detects**    | All `type: object` nodes                                                            |
| **Transforms** | `additionalProperties: false`, all props `required`, optionals â†’ `anyOf: [T, null]` |
| **Lossy**      | No â€” `Optional<T>` â‰… `T                                                             | null` |
| **Ordering**   | **Last structural pass** â€” seals all objects                                        |

### Pass 7: Constraint Pruning & Enum Sorting

| Property       | Value                                                                          |
| -------------- | ------------------------------------------------------------------------------ |
| **Detects**    | `minLength`, `maximum`, `default`, `const`, `not`, `if/then/else`, etc.        |
| **Transforms** | Drop unsupported constraints with codec entries. **Sort enums default-first.** |
| **Lossy**      | Yes for dropped constraints                                                    |

> [!TIP]
> **Round 4 â€” Enum Default-First Sorting.** Before stripping `default`, reorder `enum` to place the default value at index 0. LLMs bias toward first options when context is weak.

---

## Rehydration Codec

> [!IMPORTANT]
> **Round 4 â€” JSON Pointers for codec paths.** The converter emits schema-node pointers (e.g., `#/properties/listeners/items`). The rehydrator tracks schema position as it walks the data instance.

```json
{
  "$schema": "https://jsonschema-llm.dev/codec/v1",
  "transforms": [
    { "path": "#/properties/plans", "type": "map_to_array", "keyField": "key" },
    {
      "path": "#/properties/listeners/items/properties/configuration",
      "type": "json_string_parse"
    },
    {
      "path": "#/properties/tags",
      "type": "nullable_optional",
      "originalRequired": false
    }
  ],
  "droppedConstraints": [
    { "path": "#/properties/name", "constraint": "minLength", "value": 1 }
  ]
}
```

---

## Provider Target Matrix

| Feature                        | OpenAI Strict | Gemini           | Claude           |
| ------------------------------ | ------------- | ---------------- | ---------------- |
| `additionalProperties: false`  | Required      | Optional         | Recommended      |
| `anyOf`                        | âœ…            | âœ…               | âœ…               |
| `oneOf`                        | âŒ â†’ `anyOf`  | âœ… (skip Pass 2) | âš ï¸ â†’ `anyOf`     |
| `allOf`                        | âŒ â†’ merge    | âš ï¸ â†’ merge       | âŒ â†’ merge       |
| Recursive `$ref`               | âŒ â†’ break    | âœ… (skip Pass 5) | âš ï¸ â†’ limit depth |
| `additionalProperties: Schema` | âŒ â†’ array    | âœ… (skip Pass 3) | âŒ â†’ array       |
| `{type: object}` (opaque)      | âŒ â†’ string   | âš ï¸ â†’ string      | âŒ â†’ string      |
| `minimum`/`maximum`            | âŒ â†’ drop     | âœ… (preserve)    | âŒ â†’ drop        |
| `pattern`                      | âœ…            | âœ…               | âŒ â†’ drop        |

---

## Rust Architecture

> [!WARNING]
> **G's original suggestion of `Cow<Schema>` was not adopted.** The codebase uses `serde_json::Value` with explicit cloning. The recursive descent transformer pattern is used, but clone-on-write was unnecessary given the pass-by-pass pipeline design.

- **`serde_json::Value`** â€” All schema manipulation uses `Value` cloning for simplicity
- **Context struct** â€” e.g., `RefContext` in Pass 0 bundles root schema + config + traversal state
- **Depth Guard** â€” Configurable limit (default 50). Error on exceeded depth.
- **WASM-first** â€” Enable running converter in-browser (e.g., Gravitee Console "AI Ready" preview)

> [!NOTE]
> **Considered and deferred: `Cow<Schema>` clone-on-write.** Schema sizes are inherently bounded by LLM context windows â€” the converted schema must fit in the model's input alongside prompts and context. With practical ceilings around 64KB of schema JSON, clone-on-write would save microseconds on an operation that already runs in milliseconds. The bottleneck is always LLM inference, not the transformer.

---

## v0.1 Implementation Status

| Priority | Pass       | Name                  | Status         |
| -------- | ---------- | --------------------- | -------------- |
| 1        | Pass 6     | Strict Enforcer       | âœ… Implemented |
| 2        | Pass 3     | Dictionary Transpiler | âœ… Implemented |
| 3        | Pass 1     | Composition           | âœ… Implemented |
| 4        | Rehydrator | (Rust)                | âœ… Implemented |
| 5        | Pass 2     | Polymorphism          | âœ… Implemented |
| 6        | Pass 4     | Opaque Fallback       | âœ… Implemented |
| 7        | Pass 0     | Normalization         | âœ… Implemented |
| 8        | Pass 5     | Recursion Breaking    | ðŸ”² Stub        |
| 9        | Pass 7     | Constraint Pruning    | ðŸ”² Stub        |
| 10       | Pipeline   | `convert()` wiring    | ðŸ”² Stub        |

---

## Future Optimizations (v2+)

- **`--polymorphism=flatten`** â€” Opt-in flattening for edge cases where anyOf fails
- **Optional Grouping** â€” Bundle 10+ nullable optional fields into `options: {T | null}`
- **Description Namespacing** â€” Prepend `[HTTP]`/`[Kafka]` to merged variant field descriptions
- **Provider auto-detection** â€” Infer target from API key or `--provider` flag

---

## Project Structure

**Name**: `jsonschema-llm`

```
jsonschema-llm/
â”œâ”€â”€ crates/
â”‚   â””â”€â”€ jsonschema-llm-core/   # Rust core library
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ lib.rs          # Public API (convert + rehydrate)
â”‚           â”œâ”€â”€ passes/         # One module per pass (p0â€“p7)
â”‚           â”œâ”€â”€ codec.rs        # Codec builder
â”‚           â”œâ”€â”€ rehydrator.rs   # Reverse transforms
â”‚           â””â”€â”€ schema_utils.rs # Shared path/traversal utilities
â”œâ”€â”€ cli/                       # Rust CLI binary (stub)
â”œâ”€â”€ bindings/                  # Language bindings (not yet implemented)
â”‚   â”œâ”€â”€ typescript/            # WASM (planned)
â”‚   â”œâ”€â”€ java/                  # JNI (planned)
â”‚   â””â”€â”€ python/                # PyO3 (planned)
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ algorithm.md           # This specification
â””â”€â”€ README.md
```

---

## Validation Results (Proof of Concept)

| Test               | Input                            | Result                                                 |
| ------------------ | -------------------------------- | ------------------------------------------------------ |
| Test Schema        | 2.5KB, Maps+Discriminator+Opaque | âœ… OpenAI strict, 7/7 round-trip checks                |
| Gravitee v4 Schema | 29KB, 1216 lines, production     | âœ… OpenAI strict, 169 codec entries, round-trip passed |

**Council Session Log:**

- Round 1: T+G independent proposals â†’ T synthesis (8-pass pipeline)
- Round 2: POC implementations, OpenAI test schema validation âœ…
- Round 3: Full rehydration round-trip on test schema (7/7) + Gravitee v4 âœ…
- Round 4: Retrospective â€” `anyOf` > flattening, enum sorting, JSON Pointer paths, WASM-first, v0.1 priority
